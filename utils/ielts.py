'''
IELTS Test Mode using Netease Youdao Semantic Similarity API.
'''
import json
import logging
import os
import random
import time

import numpy as np
import requests
from sklearn.metrics.pairwise import cosine_similarity

from .base import TestBase, TestResult  # Updated import
from .config import config
from .resource_path import resource_path
from .enhanced_cache import get_enhanced_cache
from .performance_monitor import get_performance_monitor

logger = logging.getLogger(__name__)

class IeltsTest(TestBase):
    """IELTS English-to-Chinese test using semantic similarity."""
    
    # Exact translation pairs for common words
    EXACT_TRANSLATIONS = {
        'spin': ['æ—‹è½‰', 'è½‰å‹•', 'è‡ªè½‰'],
        'exceptional': ['ä¾‹å¤–çš„', 'ç‰¹æ®Šçš„', 'èˆ‡çœ¾ä¸åŒçš„', 'å“è¶Šçš„'],
        'trait': ['ç‰¹å¾µ', 'ç‰¹è³ª', 'ç‰¹æ€§'],
        'cheek': ['è‡‰é °', 'é¢é °', 'è…®'],
        'generate': ['ç”¢ç”Ÿ', 'ç”Ÿæˆ', 'å‰µé€ ', 'ç™¼ç”Ÿ'],
        'merchant': ['å•†äºº', 'å•†è²©', 'è²¿æ˜“å•†'],
        'arrangement': ['å®‰æ’', 'ä½ˆç½®', 'æ’åˆ—'],
        'unsuitable': ['ä¸åˆé©çš„', 'ä¸é©ç•¶çš„', 'ä¸æ°ç•¶çš„']
    }

    def __init__(self):
        super().__init__("IELTS è‹±è¯‘ä¸­ (è¯­ä¹‰ç›¸ä¼¼åº¦)")
        self.vocabulary = []  # ç°åœ¨å­˜å‚¨å®Œæ•´è¯æ¡å¯¹è±¡ [{word, meanings}]
        self.selected_words_for_session = []  # ç°åœ¨å­˜å‚¨å®Œæ•´è¯æ¡å¯¹è±¡
        self.current_question_index_in_session = 0
        self.embedding_cache = get_enhanced_cache()  # åˆå§‹åŒ–å¢å¼ºç¼“å­˜ç³»ç»Ÿ
        self.performance_monitor = get_performance_monitor()  # åˆå§‹åŒ–æ€§èƒ½ç›£æ§
        # self.load_vocabulary() # Vocabulary will be loaded on demand or when preparing a session

    def load_vocabulary(self):
        """Loads vocabulary from ielts_vocab.json as a list of dicts with 'word' and 'meanings'."""
        try:
            json_path = resource_path("vocab/ielts_vocab.json")
            with open(json_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
            # åªæ”¯æŒ [{"word": ..., "meanings": [...]}, ...] æ ¼å¼
            if isinstance(data, list) and all(isinstance(item, dict) and 'word' in item and 'meanings' in item for item in data):
                self.vocabulary = data
            elif isinstance(data, dict) and 'list' in data and isinstance(data['list'], list):
                # å…¼å®¹æ—§æ ¼å¼ï¼Œè‡ªåŠ¨è½¬ä¸ºæ–°æ ¼å¼
                self.vocabulary = [{"word": w, "meanings": []} for w in data['list']]
            else:
                logger.error("vocab/ielts_vocab.json åº”ä¸º [ {\"word\": ..., \"meanings\": [...] }, ... ] æ ¼å¼ã€‚")
                self.vocabulary = []
        except FileNotFoundError:
            logger.error(f"IELTS vocabulary file not found at {json_path}")
            self.vocabulary = []
        except json.JSONDecodeError:
            logger.error(f"Could not decode JSON from {json_path}. Ensure it is valid.")
            self.vocabulary = []
        except Exception as e:
            logger.error(f"Error loading IELTS vocabulary: {e}", exc_info=True)
            self.vocabulary = []
        if not self.vocabulary:
            logger.warning("IELTS vocabulary is empty. Test will have no questions.")
        else:
            logger.info(f"Loaded {len(self.vocabulary)} IELTS words.")
            # æª¢æŸ¥æ˜¯å¦éœ€è¦é ç†±ç·©å­˜
            self._check_and_preload_cache()

    def prepare_test_session(self, num_questions: int):
        """Prepares a new test session with a specified number of random word objects."""
        if not self.vocabulary:
            self.load_vocabulary()
        if not self.vocabulary:
            logger.error("IELTS vocabulary is empty. Cannot prepare test session.")
            self.selected_words_for_session = []
            self.current_question_index_in_session = 0
            return 0
        actual_num_questions = min(num_questions, len(self.vocabulary))
        if actual_num_questions <= 0:
            self.selected_words_for_session = []
            self.current_question_index_in_session = 0
            return 0
        self.selected_words_for_session = random.sample(self.vocabulary, actual_num_questions)
        self.current_question_index_in_session = 0
        logger.info(f"Prepared IELTS test session with {len(self.selected_words_for_session)} words.")
        return len(self.selected_words_for_session)

    def get_next_ielts_question(self) -> dict | None:
        """Gets the next word object for the current session. Returns None if no more questions."""
        if self.current_question_index_in_session < len(self.selected_words_for_session):
            word_obj = self.selected_words_for_session[self.current_question_index_in_session]
            return word_obj
        return None

    def get_current_session_question_count(self) -> int:
        return len(self.selected_words_for_session)

    def get_embedding(self, text: str, lang_type: str):
        """
        Gets embedding for the given text using the SiliconFlow API with caching support.
        First checks cache, then calls API if not found.
        """
        # é¦–å…ˆæª¢æŸ¥ç·©å­˜
        cached_embedding = self.embedding_cache.get(text, config.model_name)
        if cached_embedding is not None:
            logger.debug(f"ä½¿ç”¨ç·©å­˜çš„embedding: '{text[:30]}...'")
            # è¨˜éŒ„ç·©å­˜å‘½ä¸­
            self.performance_monitor.record_api_call(
                endpoint="embedding", 
                duration=0.001,  # ç·©å­˜è¨ªå•æ™‚é–“å¾ˆçŸ­
                success=True, 
                cache_hit=True
            )
            return cached_embedding
        
        # ç·©å­˜ä¸­æ²’æœ‰ï¼Œèª¿ç”¨API
        api_key = config.api_key
        if not api_key:
            logger.error("SiliconFlow API é‡‘é‘°æœªåœ¨ config.yaml ä¸­é…ç½®. IELTS è¯­ä¹‰æµ‹è¯•åŠŸèƒ½æ— æ³•ä½¿ç”¨.")
            return None

        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        payload = {
            "model": config.model_name,
            "input": text,
            "encoding_format": "float"  # As per documentation
        }

        logger.info(f"--- èª¿ç”¨ Embedding API (ç·©å­˜æœªå‘½ä¸­) ---")
        logger.info(f"URL: {config.embedding_url}")
        logger.info(f"Model: {config.model_name}")
        logger.info(f"Input text: '{text[:50]}...'") # Print first 50 chars of input

        # è¨˜éŒ„APIèª¿ç”¨é–‹å§‹æ™‚é–“
        api_start_time = time.time()
        
        try:
            response = requests.post(config.embedding_url, json=payload, headers=headers, timeout=config.api_timeout)
            response.raise_for_status()  # Raises an HTTPError for bad responses (4XX or 5XX)
            api_response = response.json()
            
            # According to the documentation, the embedding is in response_json['data'][0]['embedding']
            if api_response and 'data' in api_response and isinstance(api_response['data'], list) and len(api_response['data']) > 0:
                embedding_data = api_response['data'][0]
                if 'embedding' in embedding_data and isinstance(embedding_data['embedding'], list):
                    embedding_vector = np.array(embedding_data['embedding']).astype(np.float32)
                    if embedding_vector.ndim == 1 and embedding_vector.shape[0] > 0:
                        api_duration = time.time() - api_start_time
                        logger.info(f"Successfully retrieved embedding for '{text[:50]}...' (dimension: {embedding_vector.shape[0]})")
                        
                        # è¨˜éŒ„APIèª¿ç”¨æ€§èƒ½
                        self.performance_monitor.record_api_call(
                            endpoint="embedding", 
                            duration=api_duration, 
                            success=True, 
                            cache_hit=False
                        )
                        
                        # å°‡æ–°ç²å–çš„embeddingå­˜å…¥ç·©å­˜
                        self.embedding_cache.put(text, embedding_vector, config.model_name)
                        return embedding_vector
                    else:
                        logger.error(f"Error: Unexpected embedding vector format for '{text[:50]}...'. Vector shape: {embedding_vector.shape}")
                        return None
                else:
                    logger.error(f"Error: 'embedding' field not found or not a list in API response data for '{text[:50]}...'. Response data[0]: {embedding_data}")
                    return None
            else:
                logger.error(f"Error: Unexpected API response structure for '{text[:50]}...'. Response: {api_response}")
                return None

        except requests.exceptions.Timeout:
            logger.error(f"API request timed out for '{text[:50]}...'")
            return None
        except requests.exceptions.HTTPError as http_err:
            logger.error(f"API request failed with HTTPError for '{text[:50]}...': {http_err}. Response: {response.text}", exc_info=True)
            return None
        except requests.exceptions.RequestException as req_err:
            logger.error(f"API request failed for '{text[:50]}...': {req_err}", exc_info=True)
            return None
        except json.JSONDecodeError:
            logger.error(f"Failed to decode JSON response from API for '{text[:50]}...'. Response text: {response.text if 'response' in locals() else 'N/A'}", exc_info=True)
            return None
        except Exception as e:
            logger.error(f"An unexpected error occurred while getting embedding for '{text[:50]}...': {e}", exc_info=True)
            return None

    def check_answer_with_api(self, standard_chinese_meanings: list, user_chinese_definition: str) -> bool:
        """
        æ”¹é€²çš„ç­”æ¡ˆæª¢æŸ¥ç®—æ³•ï¼Œä½¿ç”¨å¤šå±¤åˆ¤æ–·ç­–ç•¥ï¼š
        1. æ–‡å­—å®Œå…¨åŒ¹é…
        2. é—œéµè©åŒ¹é…
        3. èªç¾©ç›¸ä¼¼åº¦ï¼ˆå‹•æ…‹é–¾å€¼ï¼‰
        4. é•·åº¦å’Œè¤‡é›œåº¦æ¬Šé‡èª¿æ•´
        """
        if not standard_chinese_meanings or not user_chinese_definition:
            logger.warning(f"ç­”æ¡ˆæª¢æŸ¥å¤±æ•—ï¼šæ¨™æº–é‡‹ç¾©æˆ–ç”¨æˆ¶ç­”æ¡ˆç‚ºç©º")
            return False
        
        logger.info(f"=== é–‹å§‹æª¢æŸ¥ç­”æ¡ˆ ===")
        logger.info(f"ç”¨æˆ¶ç­”æ¡ˆ: '{user_chinese_definition}'")
        logger.info(f"æ¨™æº–é‡‹ç¾©: {standard_chinese_meanings}")
        
        # éšæ®µ 1: æ–‡å­—å®Œå…¨åŒ¹é…
        text_match_result = self._fallback_text_matching(standard_chinese_meanings, user_chinese_definition)
        if text_match_result:
            logger.info(f"âœ… æ–‡å­—åŒ¹é…æˆåŠŸ")
            return True
        
        # éšæ®µ 2: é—œéµè©åŒ¹é… (å¦‚æœå•Ÿç”¨)
        if config.enable_keyword_matching:
            keyword_match_result = self._keyword_matching(standard_chinese_meanings, user_chinese_definition)
            if keyword_match_result:
                logger.info(f"âœ… é—œéµè©åŒ¹é…æˆåŠŸ")
                return True
        
        # æª¢æŸ¥ API é‡‘é‘°æ˜¯å¦å¯ç”¨
        if not config.api_key:
            logger.warning("API é‡‘é‘°æœªé…ç½®ï¼Œåªèƒ½ä½¿ç”¨æ–‡å­—å’Œé—œéµè©åŒ¹é…æ¨¡å¼")
            return False
        
        # éšæ®µ 3: èªç¾©ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆå‹•æ…‹é–¾å€¼ï¼‰
        logger.info("å˜—è©¦èªç¾©åŒ¹é…...")
        semantic_result = self._semantic_similarity_check(standard_chinese_meanings, user_chinese_definition)
        
        return semantic_result
    
    def _fallback_text_matching(self, standard_meanings: list, user_answer: str) -> bool:
        """
        ç•¶ API ä¸å¯ç”¨æ™‚çš„å‚™ç”¨æ–‡å­—åŒ¹é…æ–¹æ³•
        """
        user_answer = user_answer.strip()
        if not user_answer:
            return False
        
        import re
        
        for meaning in standard_meanings:
            if not meaning:
                continue
            
            # ç§»é™¤è©æ€§æ¨™è¨˜ï¼ˆå¦‚ "n.", "v.", "adj." ç­‰ï¼‰å’Œäººåæ¨™è¨˜
            cleaned_meaning = meaning
            # ç§»é™¤è©æ€§æ¨™è¨˜ - æ”¹é€²çš„æ­£è¦è¡¨é”å¼
            cleaned_meaning = re.sub(r'\b[a-zA-Z]+\.\s*', '', cleaned_meaning)
            # ç§»é™¤äººåæ¨™è¨˜ ã€åã€‘ç­‰
            cleaned_meaning = re.sub(r'ã€[^ã€‘]*ã€‘[^ï¼›ï¼Œã€‚]*', '', cleaned_meaning)
            # ç§»é™¤æ‹¬è™Ÿå…§çš„è£œå……èªªæ˜
            cleaned_meaning = re.sub(r'ï¼ˆ[^ï¼‰]*ï¼‰', '', cleaned_meaning)
            
            logger.info(f"åŸå§‹é‡‹ç¾©: '{meaning}'")
            logger.info(f"æ¸…ç†å¾Œé‡‹ç¾©: '{cleaned_meaning}'")
            
            # ç›´æ¥æª¢æŸ¥ç”¨æˆ¶ç­”æ¡ˆæ˜¯å¦åœ¨æ¸…ç†å¾Œçš„é‡‹ç¾©ä¸­
            if user_answer in cleaned_meaning:
                logger.info(f"æ–‡å­—åŒ¹é…æˆåŠŸ: '{user_answer}' ç›´æ¥åŒ…å«åœ¨æ¸…ç†å¾Œé‡‹ç¾©ä¸­")
                return True
            
            # åˆ†è©æª¢æŸ¥ï¼ˆæŒ‰æ¨™é»ç¬¦è™Ÿåˆ†å‰²ï¼‰
            # ä½¿ç”¨æ›´å…¨é¢çš„ä¸­æ–‡æ¨™é»ç¬¦è™Ÿåˆ†å‰²
            parts = re.split(r'[ï¼Œã€‚ï¼›ã€ï¼šï¼ˆï¼‰\s]+', cleaned_meaning)
            for part in parts:
                part = part.strip()
                if not part:
                    continue
                    
                logger.info(f"æª¢æŸ¥åˆ†è©ç‰‡æ®µ: '{part}'")
                
                # å®Œå…¨åŒ¹é…
                if user_answer == part:
                    logger.info(f"æ–‡å­—åŒ¹é…æˆåŠŸ: '{user_answer}' èˆ‡ '{part}' å®Œå…¨åŒ¹é…")
                    return True
                
                # åŒ…å«æª¢æŸ¥ï¼ˆé›™å‘ï¼‰
                if len(part) >= config.min_word_length:
                    if user_answer in part or part in user_answer:
                        logger.info(f"æ–‡å­—åŒ¹é…æˆåŠŸ: '{user_answer}' èˆ‡ '{part}' éƒ¨åˆ†åŒ¹é…")
                        return True
            
            # é‡å°å½¢å®¹è©çš„ç‰¹æ®Šè™•ç†ï¼ˆå¦‚"ä¸å‹å–„çš„"å’Œ"ä¸å‹å–„"ï¼‰
            if user_answer.endswith('çš„') and len(user_answer) > 2:
                base_word = user_answer[:-1]  # ç§»é™¤"çš„"
                if base_word in cleaned_meaning:
                    logger.info(f"æ–‡å­—åŒ¹é…æˆåŠŸ: '{user_answer}' (å½¢å®¹è©å½¢å¼ï¼ŒåŸºè© '{base_word}') èˆ‡æ¨™æº–é‡‹ç¾©åŒ¹é…")
                    return True
            elif not user_answer.endswith('çš„'):
                # æª¢æŸ¥æ˜¯å¦æœ‰å°æ‡‰çš„å½¢å®¹è©å½¢å¼
                adj_form = user_answer + 'çš„'
                if adj_form in cleaned_meaning:
                    logger.info(f"æ–‡å­—åŒ¹é…æˆåŠŸ: '{user_answer}' (å°æ‡‰å½¢å®¹è© '{adj_form}') èˆ‡æ¨™æº–é‡‹ç¾©åŒ¹é…")
                    return True
        
        logger.info(f"æ–‡å­—åŒ¹é…å¤±æ•—: '{user_answer}' åœ¨æ‰€æœ‰æ¨™æº–é‡‹ç¾©ä¸­æœªæ‰¾åˆ°åŒ¹é…")
        return False
    
    def _keyword_matching(self, standard_meanings: list, user_answer: str) -> bool:
        """
        æ”¹é€²çš„é—œéµè©åŒ¹é…æ–¹æ³• - ä½¿ç”¨æ›´æ™ºèƒ½çš„è©å½™åˆ†æ
        """
        user_answer = user_answer.strip()
        if len(user_answer) < 2:
            return False
        
        import re
        
        # æ“´å±•åœç”¨è©åˆ—è¡¨
        stop_words = {
            'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'èˆ‡', 'æˆ–', 'ä½†', 'è€Œ', 'ä¹Ÿ', 'éƒ½', 'å¾ˆ', 'éå¸¸', 
            'æ¯”è¼ƒ', 'ä¸€äº›', 'ä¸€å€‹', 'é€™å€‹', 'é‚£å€‹', 'ä»€éº¼', 'æ€éº¼', 'ç‚ºä»€éº¼', 'å› ç‚º', 'æ‰€ä»¥',
            'å¯ä»¥', 'èƒ½å¤ ', 'æ‡‰è©²', 'å¿…é ˆ', 'æœƒ', 'å°‡', 'æ­£åœ¨', 'å·²ç¶“', 'é‚„', 'å°±', 'æ‰', 'æ›´'
        }
        
        # é‡è¦é—œéµè©ï¼ˆçµ¦äºˆæ›´é«˜æ¬Šé‡ï¼‰
        important_keywords = set()
        # æå–ç”¨æˆ¶ç­”æ¡ˆçš„é—œéµè©
        user_keywords = self._extract_keywords(user_answer, stop_words)
        logger.info(f"ç”¨æˆ¶ç­”æ¡ˆé—œéµè©: {user_keywords}")
        
        best_overlap_ratio = 0.0
        best_meaning = ""
        
        for meaning in standard_meanings:
            if not meaning:
                continue
                
            # æ¸…ç†å’Œæå–æ¨™æº–é‡‹ç¾©é—œéµè©
            cleaned_meaning = self._clean_meaning_text(meaning)
            std_keywords = self._extract_keywords(cleaned_meaning, stop_words)
            
            if not user_keywords or not std_keywords:
                continue
            
            # è¨ˆç®—åŠ æ¬Šé‡ç–Šåº¦
            overlap_score = self._calculate_keyword_overlap_score(user_keywords, std_keywords, user_answer, cleaned_meaning)
            
            logger.info(f"æ¨™æº–é‡‹ç¾© '{meaning}' é—œéµè©åˆ†æ•¸: {overlap_score:.3f}")
            
            if overlap_score > best_overlap_ratio:
                best_overlap_ratio = overlap_score
                best_meaning = meaning
        
        # å‹•æ…‹é–¾å€¼åˆ¤æ–·
        threshold = self._get_keyword_threshold(user_answer)
        
        if best_overlap_ratio >= threshold:
            logger.info(f"âœ… é—œéµè©åŒ¹é…æˆåŠŸ: æœ€ä½³åˆ†æ•¸ {best_overlap_ratio:.3f} >= é–¾å€¼ {threshold:.3f}")
            logger.info(f"   æœ€ä½³åŒ¹é…: '{best_meaning}'")
            return True
        else:
            logger.info(f"âŒ é—œéµè©åŒ¹é…å¤±æ•—: æœ€ä½³åˆ†æ•¸ {best_overlap_ratio:.3f} < é–¾å€¼ {threshold:.3f}")
            return False
    
    def _extract_keywords(self, text: str, stop_words: set) -> set:
        """æå–æ–‡æœ¬é—œéµè©"""
        import re
        keywords = set()
        
        # å­—ç¬¦ç´šé—œéµè©ï¼ˆä¸­æ–‡ï¼‰
        for char in text:
            if char.isalpha() and char not in 'ï¼Œã€‚ï¼›ï¼šï¼ï¼Ÿï¼ˆï¼‰ã€ã€‘ã€Œã€""''':
                keywords.add(char)
        
        # è©èªç´šé—œéµè©
        words = re.split(r'[ï¼Œã€‚ï¼›ï¼šï¼ï¼Ÿï¼ˆï¼‰ã€ã€‘ã€Œã€""''\s]+', text)
        for word in words:
            word = word.strip()
            if len(word) >= 2 and word not in stop_words:
                keywords.add(word)
                
                # å°æ–¼é•·è©ï¼Œä¹Ÿæå–å­è©
                if len(word) > 3:
                    for i in range(len(word) - 1):
                        subword = word[i:i+2]
                        if subword not in stop_words:
                            keywords.add(subword)
        
        return keywords
    
    def _clean_meaning_text(self, meaning: str) -> str:
        """æ¸…ç†é‡‹ç¾©æ–‡æœ¬"""
        import re
        cleaned = meaning
        # ç§»é™¤è©æ€§æ¨™è¨˜
        cleaned = re.sub(r'\b[a-zA-Z]+\.\s*', '', cleaned)
        # ç§»é™¤äººåæ¨™è¨˜
        cleaned = re.sub(r'ã€[^ã€‘]*ã€‘[^ï¼›ï¼Œã€‚]*', '', cleaned)
        # ç§»é™¤æ‹¬è™Ÿèªªæ˜
        cleaned = re.sub(r'ï¼ˆ[^ï¼‰]*ï¼‰', '', cleaned)
        # ç§»é™¤è‹±æ–‡å–®è©
        cleaned = re.sub(r'[a-zA-Z]+', '', cleaned)
        return cleaned.strip()
    
    def _calculate_keyword_overlap_score(self, user_keywords: set, std_keywords: set, 
                                       user_text: str, std_text: str) -> float:
        """è¨ˆç®—åŠ æ¬Šé—œéµè©é‡ç–Šåˆ†æ•¸"""
        if not user_keywords or not std_keywords:
            return 0.0
        
        overlap = user_keywords.intersection(std_keywords)
        if not overlap:
            return 0.0
        
        # åŸºç¤é‡ç–Šç‡
        base_score = len(overlap) / len(user_keywords.union(std_keywords))
        
        # é•·è©æ¬Šé‡åŠ æˆ
        long_word_bonus = 0.0
        for word in overlap:
            if len(word) > 2:
                long_word_bonus += 0.1
        
        # å®Œæ•´è©åŒ¹é…åŠ æˆ
        exact_match_bonus = 0.0
        for user_word in user_keywords:
            if len(user_word) > 1 and user_word in std_text:
                exact_match_bonus += 0.15
        
        # æ ¸å¿ƒè©æ¬Šé‡ï¼ˆå¸¸è¦‹çš„é‡è¦è©å½™ï¼‰
        core_words = {'äºº', 'ç‰©', 'äº‹', 'åœ°', 'æ™‚', 'å‹•ä½œ', 'ç‹€æ…‹', 'æ€§è³ª', 'é—œä¿‚', 'æ„Ÿæƒ…', 'æ€æƒ³'}
        core_bonus = 0.0
        for word in overlap:
            if any(core in word for core in core_words):
                core_bonus += 0.05
        
        # è¨ˆç®—æœ€çµ‚åˆ†æ•¸
        final_score = base_score + long_word_bonus + exact_match_bonus + core_bonus
        return min(1.0, final_score)  # é™åˆ¶æœ€å¤§å€¼ç‚º1.0
    
    def _get_keyword_threshold(self, user_answer: str) -> float:
        """ç²å–é—œéµè©åŒ¹é…çš„å‹•æ…‹é–¾å€¼"""
        length = len(user_answer.strip())
        
        if length <= 2:
            return 0.6  # å¾ˆçŸ­çš„ç­”æ¡ˆè¦æ±‚æ›´é«˜ç²¾ç¢ºåº¦
        elif length <= 4:
            return 0.45  # çŸ­ç­”æ¡ˆ
        elif length <= 8:
            return 0.35  # ä¸­ç­‰é•·åº¦
        else:
            return 0.25  # é•·ç­”æ¡ˆå¯ä»¥ç›¸å°å¯¬é¬†
    
    def _semantic_similarity_check(self, standard_meanings: list, user_answer: str) -> bool:
        """
        æ”¹é€²çš„èªç¾©ç›¸ä¼¼åº¦æª¢æŸ¥ï¼Œçµåˆå¤šç¨®æ™ºèƒ½ç­–ç•¥
        """
        user_embedding = self.get_embedding(user_answer, lang_type="zh")
        if user_embedding is None or user_embedding.shape[0] == 0:
            logger.warning("ç”¨æˆ¶ç­”æ¡ˆ embedding ç²å–å¤±æ•—")
            return False
        
        user_embedding = user_embedding.reshape(1, -1)
        similarity_results = []
        api_success = False
        
        # ç¬¬1æ­¥ï¼šè¨ˆç®—æ‰€æœ‰ç›¸ä¼¼åº¦
        for i, std_meaning in enumerate(standard_meanings):
            if not std_meaning:
                continue
            std_embedding = self.get_embedding(std_meaning, lang_type="zh")
            if std_embedding is None or std_embedding.shape[0] == 0:
                continue
            std_embedding = std_embedding.reshape(1, -1)
            
            try:
                similarity = cosine_similarity(user_embedding, std_embedding)[0][0]
                api_success = True
                
                # è¨ˆç®—é¡å¤–çš„åŒ¹é…ä¿¡å¿ƒåˆ†æ•¸
                confidence_score = self._calculate_semantic_confidence(user_answer, std_meaning, similarity)
                
                similarity_results.append({
                    'meaning': std_meaning,
                    'similarity': similarity,
                    'confidence': confidence_score,
                    'combined_score': similarity * 0.7 + confidence_score * 0.3  # çµ„åˆåˆ†æ•¸
                })
                
                logger.info(f"èªç¾©åˆ†æ - '{user_answer}' vs '{std_meaning}'")
                logger.info(f"  ç›¸ä¼¼åº¦: {similarity:.4f}, ä¿¡å¿ƒåˆ†æ•¸: {confidence_score:.4f}, çµ„åˆåˆ†æ•¸: {similarity_results[-1]['combined_score']:.4f}")
                
            except Exception as e:
                logger.error(f"è¨ˆç®—ä½™å¼¦ç›¸ä¼¼åº¦æ™‚å‡ºéŒ¯: {e}", exc_info=True)
                continue
        
        if not api_success or not similarity_results:
            logger.warning("æ‰€æœ‰èªç¾©ç›¸ä¼¼åº¦è¨ˆç®—éƒ½å¤±æ•—")
            return False
        
        # ç¬¬2æ­¥ï¼šæ‰¾åˆ°æœ€ä½³åŒ¹é…
        best_result = max(similarity_results, key=lambda x: x['combined_score'])
        max_similarity = best_result['similarity']
        best_match_meaning = best_result['meaning']
        combined_score = best_result['combined_score']
        
        # ç¬¬3æ­¥ï¼šå¤šå±¤æ¬¡åˆ¤æ–·ç­–ç•¥
        # ç­–ç•¥1ï¼šé«˜ç›¸ä¼¼åº¦ç›´æ¥é€šé
        if max_similarity >= 0.75:
            logger.info(f"âœ… é«˜ç›¸ä¼¼åº¦ç›´æ¥é€šé: {max_similarity:.4f} >= 0.75")
            return True
        
        # ç­–ç•¥2ï¼šæª¢æŸ¥ç²¾ç¢ºç¿»è­¯åŒ¹é…
        if self._check_exact_translations(english_word, user_answer):
            return True
        
        # ç­–ç•¥3ï¼šä½¿ç”¨ç°¡åŒ–å‹•æ…‹é–¾å€¼
        if config.enable_dynamic_threshold:
            dynamic_threshold = self._calculate_simple_dynamic_threshold(user_answer, max_similarity)
        else:
            dynamic_threshold = config.similarity_threshold
        
        # ç­–ç•¥4ï¼šä½¿ç”¨çµ„åˆåˆ†æ•¸åˆ¤æ–·
        combined_threshold = dynamic_threshold * 0.9  # çµ„åˆåˆ†æ•¸çš„é–¾å€¼ç¨å¾®é™ä½
        
        logger.info(f"ğŸ“Š èªç¾©åˆ†æçµæœ:")
        logger.info(f"  æœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.4f}")
        logger.info(f"  ä¿¡å¿ƒåˆ†æ•¸: {best_result['confidence']:.4f}")
        logger.info(f"  çµ„åˆåˆ†æ•¸: {combined_score:.4f}")
        logger.info(f"  å‹•æ…‹é–¾å€¼: {dynamic_threshold:.4f}")
        logger.info(f"  çµ„åˆé–¾å€¼: {combined_threshold:.4f}")
        logger.info(f"  æœ€ä½³åŒ¹é…: '{best_match_meaning}'")
        
        # ç­–ç•¥4ï¼šç¶œåˆåˆ¤æ–·
        if combined_score >= combined_threshold:
            logger.info(f"âœ… èªç¾©åŒ¹é…æˆåŠŸï¼çµ„åˆåˆ†æ•¸ {combined_score:.4f} >= çµ„åˆé–¾å€¼ {combined_threshold:.4f}")
            return True
        elif max_similarity >= dynamic_threshold:
            logger.info(f"âœ… èªç¾©åŒ¹é…æˆåŠŸï¼ç›¸ä¼¼åº¦ {max_similarity:.4f} >= å‹•æ…‹é–¾å€¼ {dynamic_threshold:.4f}")
            return True
        else:
            logger.info(f"âŒ èªç¾©åŒ¹é…å¤±æ•—ï¼šçµ„åˆåˆ†æ•¸ {combined_score:.4f} < é–¾å€¼ {combined_threshold:.4f}")
            return False
    
    def _check_exact_translations(self, english_word: str, user_answer: str) -> bool:
        """Check against exact translation dictionary"""
        if english_word.lower() in self.EXACT_TRANSLATIONS:
            exact_translations = self.EXACT_TRANSLATIONS[english_word.lower()]
            user_clean = user_answer.strip()
            
            for translation in exact_translations:
                if user_clean == translation or user_clean in translation or translation in user_clean:
                    logger.info(f"âœ… Exact translation match: '{user_answer}' for '{english_word}'")
                    return True
        
        return False
    
    def _calculate_semantic_confidence(self, user_answer: str, std_meaning: str, similarity: float) -> float:
        """
        è¨ˆç®—èªç¾©åŒ¹é…çš„ä¿¡å¿ƒåˆ†æ•¸ï¼Œçµåˆå¤šå€‹å› ç´ 
        """
        confidence = 0.0
        
        # å› ç´ 1ï¼šé•·åº¦ç›¸ä¼¼æ€§
        len_user = len(user_answer.strip())
        len_std = len(std_meaning.strip())
        len_ratio = min(len_user, len_std) / max(len_user, len_std) if max(len_user, len_std) > 0 else 0
        length_confidence = len_ratio * 0.3
        
        # å› ç´ 2ï¼šå­—ç¬¦é‡ç–Š
        user_chars = set(user_answer)
        std_chars = set(std_meaning)
        char_overlap = len(user_chars.intersection(std_chars)) / len(user_chars.union(std_chars)) if user_chars.union(std_chars) else 0
        char_confidence = char_overlap * 0.4
        
        # å› ç´ 3ï¼šèªç¾©ç›¸ä¼¼åº¦æœ¬èº«çš„ç©©å®šæ€§
        similarity_confidence = min(similarity * 1.2, 1.0) * 0.5
        
        # å› ç´ 4ï¼šç‰¹æ®Šæ¨¡å¼åŠ æˆ
        pattern_bonus = 0.0
        
        # åŒç¾©è©æ¨¡å¼
        synonyms_pairs = [
            ('å¿«æ¨‚', 'é–‹å¿ƒ'), ('é«˜èˆˆ', 'æ„‰å¿«'), ('æ‚²å‚·', 'é›£é'),
            ('æ¼‚äº®', 'ç¾éº—'), ('è°æ˜', 'æ™ºæ…§'), ('å›°é›£', 'è‰±é›£'),
            ('å®¹æ˜“', 'ç°¡å–®'), ('é‡è¦', 'é—œéµ'), ('ç‰¹åˆ¥', 'ç‰¹æ®Š')
        ]
        
        for syn1, syn2 in synonyms_pairs:
            if (syn1 in user_answer and syn2 in std_meaning) or (syn2 in user_answer and syn1 in std_meaning):
                pattern_bonus += 0.2
                break
        
        # è©æ€§æ¨¡å¼ï¼ˆå½¢å®¹è©çš„"çš„"å­—è™•ç†ï¼‰
        if user_answer.endswith('çš„') and not std_meaning.endswith('çš„'):
            base_user = user_answer[:-1]
            if base_user in std_meaning:
                pattern_bonus += 0.15
        elif not user_answer.endswith('çš„') and std_meaning.endswith('çš„'):
            base_std = std_meaning[:-1]
            if base_std in user_answer:
                pattern_bonus += 0.15
        
        confidence = length_confidence + char_confidence + similarity_confidence + pattern_bonus
        return min(1.0, confidence)  # é™åˆ¶æœ€å¤§å€¼ç‚º1.0
    
    def _calculate_simple_dynamic_threshold(self, user_answer: str, similarity: float) -> float:
        """Simplified dynamic threshold calculation"""
        base_threshold = config.similarity_threshold
        
        # Only adjust for very short answers
        if len(user_answer.strip()) <= 2:
            return min(base_threshold * 1.1, 0.50)
        
        # For high similarity, be more lenient
        if similarity > 0.65:
            return base_threshold * 0.9
        
        return base_threshold
    
    def _get_length_factor(self, user_answer: str) -> float:
        """ç²å–é•·åº¦å› ç´ """
        user_len = len(user_answer.strip())
        if user_len <= 2:
            return 1.2
        elif user_len <= 4:
            return 1.1
        elif user_len >= 8:
            return 0.95
        return 1.0
    
    def _get_complexity_factor(self, user_answer: str) -> float:
        """ç²å–è¤‡é›œåº¦å› ç´ """
        if any(char in user_answer for char in 'ï¼Œã€‚ï¼›ï¼š'):
            return 0.95
        return 1.0
    
    def _get_trend_factor(self, similarity: float) -> float:
        """ç²å–è¶¨å‹¢å› ç´ """
        if similarity > 0.7:
            return 0.9
        elif similarity < 0.3:
            return 1.1
        return 1.0
    
    def _get_text_quality_factor(self, user_answer: str, best_match: str) -> float:
        """ç²å–æ–‡æœ¬è³ªé‡å› ç´ """
        quality_factor = 1.0
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«ç„¡æ„ç¾©å­—ç¬¦
        meaningless_chars = set('123456789abcdefghijklmnopqrstuvwxyz')
        user_chars = set(user_answer.lower())
        if meaningless_chars.intersection(user_chars):
            quality_factor *= 1.1  # åŒ…å«æ•¸å­—æˆ–è‹±æ–‡ï¼Œå¯èƒ½æ˜¯ç„¡æ„ç¾©è¼¸å…¥ï¼Œæé«˜é–¾å€¼
        
        # æª¢æŸ¥é‡è¤‡å­—ç¬¦
        if len(set(user_answer)) / len(user_answer) < 0.5:  # å­—ç¬¦é‡è¤‡åº¦é«˜
            quality_factor *= 1.05
        
        return quality_factor
    
    def _calculate_dynamic_threshold(self, user_answer: str, best_match: str, similarity: float) -> float:
        """
        è¨ˆç®—å‹•æ…‹é–¾å€¼ï¼Œè€ƒæ…®å¤šç¨®å› ç´ ï¼š
        1. ç­”æ¡ˆé•·åº¦
        2. è¤‡é›œåº¦
        3. åŸºç¤é–¾å€¼
        4. ç›¸ä¼¼åº¦è¶¨å‹¢
        """
        base_threshold = config.similarity_threshold
        
        # å› ç´ 1ï¼šé•·åº¦æ¬Šé‡
        user_len = len(user_answer.strip())
        match_len = len(best_match.strip()) if best_match else user_len
        
        length_factor = 1.0
        if user_len <= 2:  # å¾ˆçŸ­çš„ç­”æ¡ˆï¼Œè¦æ±‚æ›´é«˜
            length_factor = 1.2
        elif user_len <= 4:  # çŸ­ç­”æ¡ˆï¼Œç¨å¾®æé«˜è¦æ±‚
            length_factor = 1.1
        elif user_len >= 8:  # é•·ç­”æ¡ˆï¼Œå¯ä»¥ç¨å¾®å¯¬é¬†
            length_factor = 0.95
        
        # å› ç´ 2ï¼šè¤‡é›œåº¦æ¬Šé‡ï¼ˆæ˜¯å¦åŒ…å«å°ˆæ¥­è©å½™ã€æ¨™é»ç­‰ï¼‰
        complexity_factor = 1.0
        if any(char in user_answer for char in 'ï¼Œã€‚ï¼›ï¼š'):
            complexity_factor = 0.95  # æœ‰æ¨™é»ï¼Œç¨å¾®å¯¬é¬†
        
        # å› ç´ 3ï¼šç›¸ä¼¼åº¦è¶¨å‹¢èª¿æ•´
        trend_factor = 1.0
        if similarity > 0.7:  # é«˜ç›¸ä¼¼åº¦ï¼Œç¨å¾®å¯¬é¬†
            trend_factor = 0.9
        elif similarity < 0.3:  # ä½ç›¸ä¼¼åº¦ï¼Œç¨å¾®åš´æ ¼
            trend_factor = 1.1
        
        # è¨ˆç®—æœ€çµ‚å‹•æ…‹é–¾å€¼
        dynamic_threshold = base_threshold * length_factor * complexity_factor * trend_factor
        
        # é™åˆ¶å‹•æ…‹é–¾å€¼çš„ç¯„åœ
        dynamic_threshold = max(0.25, min(0.75, dynamic_threshold))
        
        logger.debug(f"å‹•æ…‹é–¾å€¼è¨ˆç®—: åŸºç¤={base_threshold:.3f}, é•·åº¦ä¿‚æ•¸={length_factor:.3f}, "
                    f"è¤‡é›œåº¦ä¿‚æ•¸={complexity_factor:.3f}, è¶¨å‹¢ä¿‚æ•¸={trend_factor:.3f}, "
                    f"æœ€çµ‚={dynamic_threshold:.3f}")
        
        return dynamic_threshold
    
    def _check_and_preload_cache(self):
        """æª¢æŸ¥ä¸¦é è¼‰å…¥ç·©å­˜"""
        if not config.api_key:
            logger.info("APIå¯†é‘°æœªé…ç½®ï¼Œè·³éç·©å­˜é ç†±")
            return
        
        # æª¢æŸ¥ç·©å­˜çµ±è¨ˆ
        cache_stats = self.embedding_cache.get_stats()
        cache_size = cache_stats['cache_size']
        
        # å¦‚æœç·©å­˜ç‚ºç©ºæˆ–å¾ˆå°ï¼Œæç¤ºç”¨æˆ¶æ˜¯å¦é ç†±
        if cache_size < len(self.vocabulary) * 0.5:  # å°‘æ–¼50%çš„è©å½™è¢«ç·©å­˜
            logger.info(f"ç•¶å‰ç·©å­˜å¤§å°: {cache_size}, è©å½™è¡¨å¤§å°: {len(self.vocabulary)}")
            logger.info("å»ºè­°ä½¿ç”¨ preload_cache() æ–¹æ³•é ç†±ç·©å­˜ä»¥æå‡æ€§èƒ½")
    
    def preload_cache(self, max_words: int = None, batch_size: int = 10):
        """
        é è¼‰å…¥è©å½™è¡¨çš„embeddingåˆ°ç·©å­˜
        
        Args:
            max_words: æœ€å¤§é è¼‰å…¥è©å½™æ•¸ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
            batch_size: æ‰¹æ¬¡å¤§å°ï¼Œæ§åˆ¶APIèª¿ç”¨é »ç‡
        """
        if not config.api_key:
            logger.error("APIå¯†é‘°æœªé…ç½®ï¼Œç„¡æ³•é ç†±ç·©å­˜")
            return False
        
        if not self.vocabulary:
            self.load_vocabulary()
        
        if not self.vocabulary:
            logger.error("è©å½™è¡¨ç‚ºç©ºï¼Œç„¡æ³•é ç†±ç·©å­˜")
            return False
        
        # ç¢ºå®šè¦é è¼‰å…¥çš„è©å½™
        vocab_to_preload = self.vocabulary
        if max_words and max_words < len(self.vocabulary):
            vocab_to_preload = self.vocabulary[:max_words]
        
        logger.info(f"é–‹å§‹é ç†±ç·©å­˜ï¼š{len(vocab_to_preload)} å€‹è©å½™")
        
        preloaded_count = 0
        api_calls = 0
        skipped_count = 0  # å·²å­˜åœ¨æ–¼ç·©å­˜ä¸­çš„æ•¸é‡
        
        import time
        
        for i, word_obj in enumerate(vocab_to_preload):
            word = word_obj.get('word', '')
            meanings = word_obj.get('meanings', [])
            
            if not word:
                continue
            
            try:
                # é è¼‰å…¥è‹±æ–‡å–®è©
                if self.embedding_cache.get(word) is None:
                    embedding = self.get_embedding(word, "en")
                    if embedding is not None:
                        preloaded_count += 1
                        api_calls += 1
                else:
                    skipped_count += 1
                
                # é è¼‰å…¥ä¸­æ–‡é‡‹ç¾©
                for meaning in meanings:
                    if meaning and self.embedding_cache.get(meaning) is None:
                        embedding = self.get_embedding(meaning, "zh")
                        if embedding is not None:
                            preloaded_count += 1
                            api_calls += 1
                    elif meaning:
                        skipped_count += 1
                
                # æ‰¹æ¬¡æ§åˆ¶ï¼šæ¯è™•ç†batch_sizeå€‹è©å½™å¾Œä¼‘æ¯ä¸¦æ›´æ–°çµ±è¨ˆ
                if (i + 1) % batch_size == 0:
                    progress = (i + 1) / len(vocab_to_preload) * 100
                    logger.info(f"é€²åº¦ {progress:.1f}% - å·²è™•ç† {i + 1}/{len(vocab_to_preload)} å€‹è©å½™")
                    logger.info(f"  æ–°å¢: {preloaded_count}, å·²å­˜åœ¨: {skipped_count}, APIèª¿ç”¨: {api_calls}")
                    
                    # æ‰¹æ¬¡ä¿å­˜ç·©å­˜
                    try:
                        self.embedding_cache._save_cache()
                    except Exception as e:
                        logger.warning(f"æ‰¹æ¬¡ä¿å­˜ç·©å­˜å¤±æ•—: {e}")
                    
                    time.sleep(0.5)  # é©åº¦å»¶é²ï¼Œé¿å…APIè«‹æ±‚éæ–¼é »ç¹
                    
            except Exception as e:
                logger.error(f"é è¼‰å…¥è©å½™ '{word}' æ™‚å‡ºéŒ¯: {e}")
                continue
        
        # æœ€çµ‚ä¿å­˜ç·©å­˜
        try:
            self.embedding_cache._save_cache()
        except Exception as e:
            logger.warning(f"æœ€çµ‚ä¿å­˜ç·©å­˜å¤±æ•—: {e}")
        
        # ç²å–æœ€çµ‚çµ±è¨ˆ
        final_stats = self.embedding_cache.get_stats()
        
        logger.info(f"ğŸ‰ ç·©å­˜é ç†±å®Œæˆï¼")
        logger.info(f"  ğŸ“Š çµ±è¨ˆä¿¡æ¯:")
        logger.info(f"    æ–°å¢embedding: {preloaded_count}")
        logger.info(f"    è·³é(å·²å­˜åœ¨): {skipped_count}")
        logger.info(f"    APIèª¿ç”¨æ¬¡æ•¸: {api_calls}")
        logger.info(f"    ç·©å­˜ç¸½å¤§å°: {final_stats['cache_size']}")
        logger.info(f"    ç•¶å‰å‘½ä¸­ç‡: {final_stats['hit_rate']}")
        
        # è¨ˆç®—æ•ˆç‡æå‡é ä¼°
        total_possible_items = len(vocab_to_preload) * 2  # è‹±æ–‡+ä¸­æ–‡
        coverage_rate = final_stats['cache_size'] / total_possible_items * 100 if total_possible_items > 0 else 0
        logger.info(f"    è©å½™è¦†è“‹ç‡: {coverage_rate:.1f}%")
        
        if coverage_rate > 80:
            logger.info("âœ… ç·©å­˜é ç†±æ•ˆæœè‰¯å¥½ï¼Œæ¸¬è©¦éŸ¿æ‡‰é€Ÿåº¦å°‡å¤§å¹…æå‡ï¼")
        elif coverage_rate > 50:
            logger.info("âš¡ ç·©å­˜é ç†±æ•ˆæœä¸­ç­‰ï¼Œå»ºè­°ç¹¼çºŒé ç†±æ›´å¤šè©å½™")
        else:
            logger.info("âš ï¸  ç·©å­˜è¦†è“‹ç‡è¼ƒä½ï¼Œå¯è€ƒæ…®å¢åŠ é ç†±è©å½™æ•¸é‡")
        
        return True
    
    def get_cache_info(self):
        """ç²å–ç·©å­˜ä¿¡æ¯"""
        stats = self.embedding_cache.get_stats()
        vocab_size = len(self.vocabulary) if self.vocabulary else 0
        
        return {
            "cache_size": stats['cache_size'],
            "vocabulary_size": vocab_size,
            "coverage_rate": f"{stats['cache_size'] / (vocab_size * 2) * 100:.1f}%" if vocab_size > 0 else "0%",
            "hit_rate": stats['hit_rate'],
            "hits": stats['hits'],
            "misses": stats['misses']
        }

    def run_test(self, num_questions: int, on_question_display, on_result_display):
        """è¿è¡ŒIELTSæµ‹è¯•ä¼šè¯ã€‚"""
        # é–‹å§‹æ€§èƒ½ç›£æ§æœƒè©±
        self.performance_monitor.start_session("IELTS è‹±è¯‘ä¸­")
        
        if not self.vocabulary:
            on_result_display("é”™è¯¯ï¼šIELTSè¯æ±‡è¡¨ä¸ºç©ºæˆ–åŠ è½½å¤±è´¥ï¼Œæ— æ³•å¼€å§‹æµ‹è¯•ã€‚", True, 0, 0, 0, 0, [])
            return

        actual_num_questions = min(num_questions, len(self.vocabulary))
        if actual_num_questions <= 0:
            on_result_display("é”™è¯¯ï¼šæ²¡æœ‰è¶³å¤Ÿçš„è¯æ±‡è¿›è¡Œæµ‹è¯•ï¼ˆæˆ–è¯·æ±‚æ•°é‡ä¸º0ï¼‰ã€‚", True, 0, 0, 0, 0, [])
            return
            
        # ç¡®ä¿ random.sample ä¸ä¼šè¶…å‡ºèŒƒå›´
        if len(self.vocabulary) < actual_num_questions:
            logger.warning(f"è¯·æ±‚äº†{actual_num_questions}é¢˜ï¼Œä½†åªæœ‰{len(self.vocabulary)}ä¸ªå•è¯ã€‚å°†ä½¿ç”¨å…¨éƒ¨å•è¯ã€‚")
            actual_num_questions = len(self.vocabulary)

        selected_words = random.sample(self.vocabulary, actual_num_questions)
        
        # è¯»å–å®Œæ•´çš„è¯æ¡å¯¹è±¡åˆ—è¡¨ï¼ˆå¸¦ meaningsï¼‰
        json_path = resource_path("vocab/ielts_vocab.json")
        with open(json_path, 'r', encoding='utf-8') as file:
            vocab_data = json.load(file)
        word2meanings = {}
        if isinstance(vocab_data, list):
            for item in vocab_data:
                if isinstance(item, dict) and 'word' in item and 'meanings' in item:
                    word2meanings[item['word']] = item['meanings']
        
        correct_answers = 0
        incorrect_answers = 0
        skipped_answers = 0
        detailed_results = []

        for i, word_obj in enumerate(selected_words):
            english_word = word_obj['word']
            meanings = word_obj.get('meanings', [])
            question_text = f"è‹±æ–‡å•è¯ï¼š {english_word}\n\nè¯·è¾“å…¥æ‚¨çš„ä¸­æ–‡ç¿»è¯‘ï¼š"
            user_input = on_question_display(
                question_text, 
                i + 1, 
                actual_num_questions,
                is_semantic_test=True
            )
            if not meanings or not any(meanings):
                ref_answer = "ï¼ˆæ— ä¸­æ–‡é‡Šä¹‰ï¼‰"
            else:
                ref_answer = "ï¼›".join([m for m in meanings if m])
            if user_input is None:  # è·³è¿‡
                skipped_answers += 1
                result = TestResult(
                    question_num=i + 1,
                    question=english_word,
                    expected_answer=ref_answer,
                    user_answer="è·³è¿‡",
                    is_correct=False,
                    notes="ç”¨æˆ·è·³è¿‡"
                )
            elif not user_input.strip(): # ç©ºç­”æ¡ˆè§†ä¸ºé”™è¯¯
                incorrect_answers += 1
                result = TestResult(
                    question_num=i + 1,
                    question=english_word,
                    expected_answer=ref_answer,
                    user_answer=user_input if user_input else "<ç©º>",
                    is_correct=False,
                    notes="ç­”æ¡ˆä¸ºç©º"
                )
            else:
                is_correct = self.check_answer_with_api(meanings, user_input.strip())
                # è¨˜éŒ„ç­”é¡Œçµæœ
                self.performance_monitor.record_question_answered(is_correct)
                if is_correct:
                    correct_answers += 1
                else:
                    incorrect_answers += 1
                result = TestResult(
                    question_num=i + 1,
                    question=english_word,
                    expected_answer=ref_answer,
                    user_answer=user_input,
                    is_correct=is_correct,
                    notes=f"è¯­ä¹‰ç›¸ä¼¼åº¦åˆ¤å®š"
                )
            detailed_results.append(result)

        total_answered = correct_answers + incorrect_answers
        accuracy = (correct_answers / total_answered * 100) if total_answered > 0 else 0
        
        summary_message = (
            f"IELTSæµ‹è¯•å®Œæˆ!\n\n"
            f"æ€»é¢˜æ•°: {actual_num_questions}\n"
            f"å›ç­”æ­£ç¡®: {correct_answers}\n"
            f"å›ç­”é”™è¯¯: {incorrect_answers}\n"
            f"è·³è¿‡é¢˜æ•°: {skipped_answers}\n"
            f"å‡†ç¡®ç‡: {accuracy:.2f}% (åŸºäºå·²å›ç­”é¢˜ç›®)\n\n"
            f"æ³¨æ„ï¼šæ­¤æµ‹è¯•é€šè¿‡è¯­ä¹‰ç›¸ä¼¼åº¦åˆ¤æ–­ç­”æ¡ˆï¼Œé˜ˆå€¼ä¸º {config.similarity_threshold}ã€‚"
        )
        
        # çµæŸæ€§èƒ½ç›£æ§æœƒè©±
        self.performance_monitor.end_session()
        
        # æ›´æ–°ç·©å­˜çµ±è¨ˆ
        cache_info = self.get_cache_info()
        self.performance_monitor.update_cache_stats(cache_info)
        
        on_result_display(
            summary_message, 
            False, 
            actual_num_questions, 
            correct_answers, 
            incorrect_answers, 
            skipped_answers, 
            detailed_results
        )

    def get_vocabulary_size(self) -> int:
        return len(self.vocabulary)

    def start(self, num_questions: int | None = None):
        """Runs the IELTS test in CLI mode."""
        if not self.vocabulary:
            self.load_vocabulary()
        
        if not self.vocabulary:
            logger.error("é”™è¯¯ï¼šIELTS è¯æ±‡è¡¨ä¸ºç©ºåŠ è½½å¤±è´¥ï¼Œæ— æ³•å¼€å§‹æµ‹è¯•ã€‚")
            return

        if num_questions is None:
            while True:
                try:
                    num_input = input(f"è¯·è¾“å…¥æµ‹è¯•é¢˜æ•° (1-{len(self.vocabulary)}ï¼Œé»˜è®¤ä¸º10ï¼Œç›´æ¥æŒ‰ Enter ä½¿ç”¨é»˜è®¤å€¼): ").strip()
                    if not num_input:
                        num_questions = 10
                        break
                    num_questions = int(num_input)
                    if 1 <= num_questions <= len(self.vocabulary):
                        break
                    else:
                        # This is a CLI user prompt, so print is appropriate
                        print(f"è¯·è¾“å…¥1åˆ°{len(self.vocabulary)}ä¹‹é—´çš„æœ‰æ•ˆæ•°å­—ã€‚")
                except ValueError:
                    # This is a CLI user prompt, so print is appropriate
                    print("è¾“å…¥æ— æ•ˆï¼Œè¯·è¾“å…¥ä¸€ä¸ªæ•°å­—ã€‚")
        
        num_questions = min(max(1, num_questions), len(self.vocabulary))

        prepared_count = self.prepare_test_session(num_questions)
        if prepared_count == 0:
            logger.error("é”™è¯¯ï¼šæ— æ³•å‡†å¤‡IELTSæµ‹è¯•ä¼šè¯ (å¯èƒ½æ˜¯è¯æ±‡è¡¨ä¸ºç©ºæˆ–æ•°é‡ä¸è¶³)ã€‚")
            return

        # This is a CLI status message, print is appropriate
        print(f"--- å¼€å§‹IELTSè‹±è¯‘ä¸­æµ‹è¯• (å…± {prepared_count} é¢˜) ---")
        correct_answers = 0
        detailed_results_cli = []

        for i, word_obj in enumerate(self.selected_words_for_session):
            eng_word = word_obj['word']
            meanings = word_obj.get('meanings', [])
            print(f"\né¢˜ç›® {i+1}/{prepared_count}: {eng_word}")
            # CLI user interaction - print is appropriate
            user_chinese = input("è¯·è¾“å…¥æ‚¨çš„ä¸­æ–‡ç¿»è¯‘: ").strip()
            is_correct = False
            if not user_chinese:
                # CLI user feedback - print is appropriate
                print("æç¤ºï¼šç­”æ¡ˆä¸èƒ½ä¸ºç©ºï¼Œè®¡ä¸ºé”™è¯¯ã€‚")
            else:
                is_correct = self.check_answer_with_api(meanings, user_chinese)
            if is_correct:
                # CLI user feedback - print is appropriate
                print("å›ç­”æ­£ç¡®ï¼")
                correct_answers += 1
            else:
                # CLI user feedback - print is appropriate
                print("å›ç­”é”™è¯¯æˆ–è¯­ä¹‰ä¸ç¬¦ã€‚")
            detailed_results_cli.append(
                TestResult(
                    question_num=i + 1,
                    question=eng_word,
                    expected_answer=f"è¯­ä¹‰ç›¸ä¼¼åº¦ > {config.similarity_threshold}",
                    user_answer=user_chinese if user_chinese else "<ç©º>",
                    is_correct=is_correct,
                    notes="è¯­ä¹‰ç›¸ä¼¼åº¦åˆ¤å®š (CLI)"
                )
            )

        print("\n" + "="*20 + " æµ‹è¯•ç»“æŸ " + "="*20)
        print(f"æ€»é¢˜æ•°: {prepared_count}")
        print(f"å›ç­”æ­£ç¡®: {correct_answers}")
        print(f"å›ç­”é”™è¯¯: {prepared_count - correct_answers}")
        accuracy = (correct_answers / prepared_count * 100) if prepared_count > 0 else 0
        # CLI summary - print is appropriate
        print(f"å‡†ç¡®ç‡: {accuracy:.2f}%")
        print(f"(è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼: {config.similarity_threshold})")

        # Optionally, display wrong answers - CLI output
        if correct_answers < prepared_count:
            print("\n--- é”™è¯¯é¢˜ç›®å›é¡¾ ---")
            for res in detailed_results_cli:
                if not res.is_correct:
                    print(f"é¢˜ç›®: {res.question}")
                    print(f"  æ‚¨çš„ç­”æ¡ˆ: {res.user_answer}")
                    print(f"  åˆ¤å®šæ ‡å‡†: {res.expected_answer}")
        print("="*50)
