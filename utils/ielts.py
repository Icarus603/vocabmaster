'''
IELTS Test Mode using Netease Youdao Semantic Similarity API.
'''
import json
import logging
import os
import random
import time

import numpy as np
import requests
from sklearn.metrics.pairwise import cosine_similarity

from .base import TestBase, TestResult  # Updated import
from .config import config
from .enhanced_cache import get_enhanced_cache
from .performance_monitor import get_performance_monitor
from .resource_path import resource_path

logger = logging.getLogger(__name__)

class IeltsTest(TestBase):
    """IELTS English-to-Chinese test using semantic similarity."""
    
    # Exact translation pairs for common words
    EXACT_TRANSLATIONS = {
        'spin': ['æ—‹è½¬', 'è½¬åŠ¨', 'è‡ªè½¬'],
        'exceptional': ['ä¾‹å¤–çš„', 'ç‰¹æ®Šçš„', 'ä¸ä¼—ä¸åŒçš„', 'å“è¶Šçš„'],
        'trait': ['ç‰¹å¾', 'ç‰¹è´¨', 'ç‰¹æ€§'],
        'cheek': ['è„¸é¢Š', 'é¢é¢Š', 'è…®'],
        'generate': ['äº§ç”Ÿ', 'ç”Ÿæˆ', 'åˆ›é€ ', 'å‘ç”Ÿ'],
        'merchant': ['å•†äºº', 'å•†è´©', 'è´¸æ˜“å•†'],
        'arrangement': ['å®‰æ’', 'å¸ƒç½®', 'æ’åˆ—'],
        'unsuitable': ['ä¸åˆé€‚çš„', 'ä¸é€‚å½“çš„', 'ä¸æ°å½“çš„']
    }

    def __init__(self):
        super().__init__("IELTS è‹±è¯‘ä¸­ (è¯­ä¹‰ç›¸ä¼¼åº¦)")
        self.vocabulary = []  # ç°åœ¨å­˜å‚¨å®Œæ•´è¯æ¡å¯¹è±¡ [{word, meanings}]
        self.selected_words_for_session = []  # ç°åœ¨å­˜å‚¨å®Œæ•´è¯æ¡å¯¹è±¡
        self.current_question_index_in_session = 0
        self.embedding_cache = get_enhanced_cache()  # åˆå§‹åŒ–å¢å¼ºç¼“å­˜ç³»ç»Ÿ
        self.performance_monitor = get_performance_monitor()  # åˆå§‹åŒ–æ€§èƒ½ç›‘æ§
        # self.load_vocabulary() # Vocabulary will be loaded on demand or when preparing a session

    def load_vocabulary(self):
        """Loads vocabulary from ielts_vocab.json as a list of dicts with 'word' and 'meanings'."""
        try:
            json_path = resource_path("vocab/ielts_vocab.json")
            with open(json_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
            # åªæ”¯æŒ [{"word": ..., "meanings": [...]}, ...] æ ¼å¼
            if isinstance(data, list) and all(isinstance(item, dict) and 'word' in item and 'meanings' in item for item in data):
                self.vocabulary = data
            elif isinstance(data, dict) and 'list' in data and isinstance(data['list'], list):
                # å…¼å®¹æ—§æ ¼å¼ï¼Œè‡ªåŠ¨è½¬ä¸ºæ–°æ ¼å¼
                self.vocabulary = [{"word": w, "meanings": []} for w in data['list']]
            else:
                logger.error("vocab/ielts_vocab.json åº”ä¸º [ {\"word\": ..., \"meanings\": [...] }, ... ] æ ¼å¼ã€‚")
                self.vocabulary = []
        except FileNotFoundError:
            logger.error(f"IELTS vocabulary file not found at {json_path}")
            self.vocabulary = []
        except json.JSONDecodeError:
            logger.error(f"Could not decode JSON from {json_path}. Ensure it is valid.")
            self.vocabulary = []
        except Exception as e:
            logger.error(f"Error loading IELTS vocabulary: {e}", exc_info=True)
            self.vocabulary = []
        if not self.vocabulary:
            logger.warning("IELTS vocabulary is empty. Test will have no questions.")
        else:
            logger.info(f"Loaded {len(self.vocabulary)} IELTS words.")
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é¢„çƒ­ç¼“å­˜
            self._check_and_preload_cache()

    def prepare_test_session(self, num_questions: int):
        """Prepares a new test session with a specified number of random word objects."""
        if not self.vocabulary:
            self.load_vocabulary()
        if not self.vocabulary:
            logger.error("IELTS vocabulary is empty. Cannot prepare test session.")
            self.selected_words_for_session = []
            self.current_question_index_in_session = 0
            return 0
        actual_num_questions = min(num_questions, len(self.vocabulary))
        if actual_num_questions <= 0:
            self.selected_words_for_session = []
            self.current_question_index_in_session = 0
            return 0
        self.selected_words_for_session = random.sample(self.vocabulary, actual_num_questions)
        self.current_question_index_in_session = 0
        logger.info(f"Prepared IELTS test session with {len(self.selected_words_for_session)} words.")
        return len(self.selected_words_for_session)

    def get_next_ielts_question(self) -> dict | None:
        """Gets the next word object for the current session. Returns None if no more questions."""
        if self.current_question_index_in_session < len(self.selected_words_for_session):
            word_obj = self.selected_words_for_session[self.current_question_index_in_session]
            return word_obj
        return None

    def get_current_session_question_count(self) -> int:
        return len(self.selected_words_for_session)

    def get_embedding(self, text: str, lang_type: str):
        """
        Gets embedding for the given text using the SiliconFlow API with caching support.
        First checks cache, then calls API if not found.
        """
        # é¦–å…ˆæ£€æŸ¥ç¼“å­˜
        cached_embedding = self.embedding_cache.get(text, config.model_name)
        if cached_embedding is not None:
            logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„embedding: '{text[:30]}...'")
            # è®°å½•ç¼“å­˜å‘½ä¸­
            self.performance_monitor.record_api_call(
                endpoint="embedding", 
                duration=0.001,  # ç¼“å­˜è®¿é—®æ—¶é—´å¾ˆçŸ­
                success=True, 
                cache_hit=True
            )
            return cached_embedding
        
        # ç¼“å­˜ä¸­æ²¡æœ‰ï¼Œè°ƒç”¨API
        api_key = config.api_key
        if not api_key:
            logger.error("SiliconFlow API å¯†é’¥æœªåœ¨ config.yaml ä¸­é…ç½®. IELTS è¯­ä¹‰æµ‹è¯•åŠŸèƒ½æ— æ³•ä½¿ç”¨.")
            return None

        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        payload = {
            "model": config.model_name,
            "input": text,
            "encoding_format": "float"  # As per documentation
        }

        logger.info(f"--- è°ƒç”¨ Embedding API (ç¼“å­˜æœªå‘½ä¸­) ---")
        logger.info(f"URL: {config.embedding_url}")
        logger.info(f"Model: {config.model_name}")
        logger.info(f"Input text: '{text[:50]}...'") # Print first 50 chars of input

        # è®°å½•APIè°ƒç”¨å¼€å§‹æ—¶é—´
        api_start_time = time.time()
        
        try:
            response = requests.post(config.embedding_url, json=payload, headers=headers, timeout=config.api_timeout)
            response.raise_for_status()  # Raises an HTTPError for bad responses (4XX or 5XX)
            api_response = response.json()
            
            # According to the documentation, the embedding is in response_json['data'][0]['embedding']
            if api_response and 'data' in api_response and isinstance(api_response['data'], list) and len(api_response['data']) > 0:
                embedding_data = api_response['data'][0]
                if 'embedding' in embedding_data and isinstance(embedding_data['embedding'], list):
                    embedding_vector = np.array(embedding_data['embedding']).astype(np.float32)
                    if embedding_vector.ndim == 1 and embedding_vector.shape[0] > 0:
                        api_duration = time.time() - api_start_time
                        logger.info(f"Successfully retrieved embedding for '{text[:50]}...' (dimension: {embedding_vector.shape[0]})")
                        
                        # è®°å½•APIè°ƒç”¨æ€§èƒ½
                        self.performance_monitor.record_api_call(
                            endpoint="embedding", 
                            duration=api_duration, 
                            success=True, 
                            cache_hit=False
                        )
                        
                        # å°†æ–°è·å–çš„embeddingå­˜å…¥ç¼“å­˜
                        self.embedding_cache.put(text, embedding_vector, config.model_name)
                        return embedding_vector
                    else:
                        logger.error(f"Error: Unexpected embedding vector format for '{text[:50]}...'. Vector shape: {embedding_vector.shape}")
                        return None
                else:
                    logger.error(f"Error: 'embedding' field not found or not a list in API response data for '{text[:50]}...'. Response data[0]: {embedding_data}")
                    return None
            else:
                logger.error(f"Error: Unexpected API response structure for '{text[:50]}...'. Response: {api_response}")
                return None

        except requests.exceptions.Timeout:
            logger.error(f"API request timed out for '{text[:50]}...'")
            return None
        except requests.exceptions.HTTPError as http_err:
            logger.error(f"API request failed with HTTPError for '{text[:50]}...': {http_err}. Response: {response.text}", exc_info=True)
            return None
        except requests.exceptions.RequestException as req_err:
            logger.error(f"API request failed for '{text[:50]}...': {req_err}", exc_info=True)
            return None
        except json.JSONDecodeError:
            logger.error(f"Failed to decode JSON response from API for '{text[:50]}...'. Response text: {response.text if 'response' in locals() else 'N/A'}", exc_info=True)
            return None
        except Exception as e:
            logger.error(f"An unexpected error occurred while getting embedding for '{text[:50]}...': {e}", exc_info=True)
            return None

    def check_answer_with_api(self, standard_chinese_meanings: list, user_chinese_definition: str) -> bool:
        """
        æ”¹è¿›çš„ç­”æ¡ˆæ£€æŸ¥ç®—æ³•ï¼š
        1. å®Œå…¨åŒ¹é…æ£€æŸ¥ï¼ˆä¸€æ¨¡ä¸€æ ·çš„ä¸­æ–‡é‡Šä¹‰ï¼‰
        2. è¯­ä¹‰ç›¸ä¼¼åº¦æ£€æŸ¥ï¼ˆembeddingæ¯”å¯¹ï¼‰
        """
        if not standard_chinese_meanings or not user_chinese_definition:
            logger.warning(f"ç­”æ¡ˆæ£€æŸ¥å¤±è´¥ï¼šæ ‡å‡†é‡Šä¹‰æˆ–ç”¨æˆ·ç­”æ¡ˆä¸ºç©º")
            return False
        
        logger.info(f"=== å¼€å§‹æ£€æŸ¥ç­”æ¡ˆ ===")
        logger.info(f"ç”¨æˆ·ç­”æ¡ˆ: '{user_chinese_definition}'")
        logger.info(f"æ ‡å‡†é‡Šä¹‰: {standard_chinese_meanings}")
        
        # é˜¶æ®µ 1: å®Œå…¨åŒ¹é…æ£€æŸ¥
        exact_match_result = self._exact_match_check(standard_chinese_meanings, user_chinese_definition)
        if exact_match_result:
            logger.info(f"âœ… å®Œå…¨åŒ¹é…æˆåŠŸ")
            return True
        
        # æ£€æŸ¥ API å¯†é’¥æ˜¯å¦å¯ç”¨
        if not config.api_key:
            logger.warning("API å¯†é’¥æœªé…ç½®ï¼Œåªèƒ½ä½¿ç”¨å®Œå…¨åŒ¹é…æ¨¡å¼")
            return False
        
        # é˜¶æ®µ 2: è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…
        logger.info("å°è¯•è¯­ä¹‰åŒ¹é…...")
        semantic_result = self._semantic_similarity_check(standard_chinese_meanings, user_chinese_definition)
        
        return semantic_result
    
    def _exact_match_check(self, standard_meanings: list, user_answer: str) -> bool:
        """
        å®Œå…¨åŒ¹é…æ£€æŸ¥ï¼šæ£€æŸ¥ç”¨æˆ·ç­”æ¡ˆæ˜¯å¦ä¸æ ‡å‡†é‡Šä¹‰å®Œå…¨ä¸€è‡´
        """
        user_answer = user_answer.strip()
        if not user_answer:
            return False
        
        import re
        
        for meaning in standard_meanings:
            if not meaning:
                continue
            
            # ç§»é™¤è¯æ€§æ ‡è®°ï¼ˆå¦‚ "n.", "v.", "adj." ç­‰ï¼‰
            cleaned_meaning = meaning
            cleaned_meaning = re.sub(r'\b[a-zA-Z]+\.\s*', '', cleaned_meaning)
            # ç§»é™¤æ‹¬å·å†…çš„è¡¥å……è¯´æ˜
            cleaned_meaning = re.sub(r'ï¼ˆ[^ï¼‰]*ï¼‰', '', cleaned_meaning)
            cleaned_meaning = re.sub(r'\([^)]*\)', '', cleaned_meaning)
            
            logger.info(f"åŸå§‹é‡Šä¹‰: '{meaning}'")
            logger.info(f"æ¸…ç†åé‡Šä¹‰: '{cleaned_meaning}'")
            
            # æŒ‰æ ‡ç‚¹ç¬¦å·åˆ†å‰²æˆç‹¬ç«‹çš„é‡Šä¹‰ç‰‡æ®µ
            parts = re.split(r'[ï¼Œã€‚ï¼›ã€ï¼š]+', cleaned_meaning)
            for part in parts:
                part = part.strip()
                if not part:
                    continue
                    
                logger.info(f"æ£€æŸ¥é‡Šä¹‰ç‰‡æ®µ: '{part}'")
                
                # å®Œå…¨åŒ¹é…æ£€æŸ¥
                if user_answer == part:
                    logger.info(f"å®Œå…¨åŒ¹é…æˆåŠŸ: '{user_answer}' ä¸ '{part}' å®Œå…¨ä¸€è‡´")
                    return True
        
        logger.info(f"å®Œå…¨åŒ¹é…å¤±è´¥: '{user_answer}' ä¸æ‰€æœ‰æ ‡å‡†é‡Šä¹‰éƒ½ä¸å®Œå…¨ä¸€è‡´")
        return False
    
    def _semantic_similarity_check(self, standard_meanings: list, user_answer: str) -> bool:
        """
        æ”¹è¿›çš„è¯­ä¹‰ç›¸ä¼¼åº¦æ£€æŸ¥ï¼ŒåŸºäºæ›´ç§‘å­¦çš„NLPæ–¹æ³•
        """
        user_embedding = self.get_embedding(user_answer, lang_type="zh")
        if user_embedding is None or user_embedding.shape[0] == 0:
            logger.warning("ç”¨æˆ·ç­”æ¡ˆ embedding è·å–å¤±è´¥")
            return False
        
        user_embedding = user_embedding.reshape(1, -1)
        similarity_results = []
        api_success = False
        
        # ç¬¬1æ­¥ï¼šé¢„å¤„ç†ç”¨æˆ·ç­”æ¡ˆ
        normalized_user_answer = self._normalize_chinese_text(user_answer)
        
        # ç¬¬2æ­¥ï¼šè®¡ç®—æ‰€æœ‰ç›¸ä¼¼åº¦
        for i, std_meaning in enumerate(standard_meanings):
            if not std_meaning:
                continue
            std_embedding = self.get_embedding(std_meaning, lang_type="zh")
            if std_embedding is None or std_embedding.shape[0] == 0:
                continue
            std_embedding = std_embedding.reshape(1, -1)
            
            try:
                similarity = cosine_similarity(user_embedding, std_embedding)[0][0]
                api_success = True
                
                # é¢„å¤„ç†æ ‡å‡†é‡Šä¹‰
                normalized_std_meaning = self._normalize_chinese_text(std_meaning)
                
                # è®¡ç®—è¯­è¨€å­¦å¢å¼ºåˆ†æ•°
                linguistic_score = self._calculate_linguistic_similarity(
                    normalized_user_answer, normalized_std_meaning
                )
                
                # ç»„åˆåˆ†æ•°ï¼šè¯­ä¹‰ç›¸ä¼¼åº¦ä¸ºä¸»ï¼Œè¯­è¨€å­¦ç‰¹å¾ä¸ºè¾…
                combined_score = similarity * 0.8 + linguistic_score * 0.2
                
                similarity_results.append({
                    'meaning': std_meaning,
                    'similarity': similarity,
                    'linguistic_score': linguistic_score,
                    'combined_score': combined_score
                })
                
                logger.info(f"è¯­ä¹‰åˆ†æ - '{user_answer}' vs '{std_meaning}'")
                logger.info(f"  ä½™å¼¦ç›¸ä¼¼åº¦: {similarity:.4f}")
                logger.info(f"  è¯­è¨€å­¦åˆ†æ•°: {linguistic_score:.4f}")
                logger.info(f"  ç»„åˆåˆ†æ•°: {combined_score:.4f}")
                
            except Exception as e:
                logger.error(f"è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦æ—¶å‡ºé”™: {e}", exc_info=True)
                continue
        
        if not api_success or not similarity_results:
            logger.warning("æ‰€æœ‰è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—éƒ½å¤±è´¥")
            return False
        
        # ç¬¬3æ­¥ï¼šæ‰¾åˆ°æœ€ä½³åŒ¹é…
        best_result = max(similarity_results, key=lambda x: x['combined_score'])
        max_similarity = best_result['similarity']
        best_match_meaning = best_result['meaning']
        combined_score = best_result['combined_score']
        linguistic_score = best_result['linguistic_score']
        
        # ç¬¬4æ­¥ï¼šç§‘å­¦çš„é˜ˆå€¼åˆ¤æ–­
        base_threshold = config.similarity_threshold
        
        # ç®€åŒ–çš„åŠ¨æ€é˜ˆå€¼ï¼šåªè€ƒè™‘ç­”æ¡ˆè´¨é‡
        quality_penalty = self._calculate_answer_quality_penalty(user_answer)
        adjusted_threshold = base_threshold + quality_penalty
        
        # å¯¹äºç»„åˆåˆ†æ•°ï¼Œä½¿ç”¨ç¨ä½çš„é˜ˆå€¼
        combined_threshold = adjusted_threshold * 0.9
        
        logger.info(f"ğŸ“Š è¯­ä¹‰åˆ†æç»“æœ:")
        logger.info(f"  æœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.4f}")
        logger.info(f"  è¯­è¨€å­¦åˆ†æ•°: {linguistic_score:.4f}")
        logger.info(f"  ç»„åˆåˆ†æ•°: {combined_score:.4f}")
        logger.info(f"  åŸºç¡€é˜ˆå€¼: {base_threshold:.4f}")
        logger.info(f"  è´¨é‡æƒ©ç½š: {quality_penalty:.4f}")
        logger.info(f"  è°ƒæ•´é˜ˆå€¼: {adjusted_threshold:.4f}")
        logger.info(f"  ç»„åˆé˜ˆå€¼: {combined_threshold:.4f}")
        logger.info(f"  æœ€ä½³åŒ¹é…: '{best_match_meaning}'")
        
        # ç¬¬5æ­¥ï¼šå¤šå±‚åˆ¤æ–­
        # é«˜ç›¸ä¼¼åº¦ç›´æ¥é€šè¿‡
        if max_similarity >= 0.75:
            logger.info(f"âœ… é«˜ç›¸ä¼¼åº¦ç›´æ¥é€šè¿‡: {max_similarity:.4f} >= 0.75")
            return True
        
        # ç»„åˆåˆ†æ•°åˆ¤æ–­
        if combined_score >= combined_threshold:
            logger.info(f"âœ… è¯­ä¹‰åŒ¹é…æˆåŠŸï¼ç»„åˆåˆ†æ•° {combined_score:.4f} >= ç»„åˆé˜ˆå€¼ {combined_threshold:.4f}")
            return True
        
        # çº¯ç›¸ä¼¼åº¦åˆ¤æ–­ï¼ˆå¤‡ç”¨ï¼‰
        if max_similarity >= adjusted_threshold:
            logger.info(f"âœ… è¯­ä¹‰åŒ¹é…æˆåŠŸï¼ç›¸ä¼¼åº¦ {max_similarity:.4f} >= è°ƒæ•´é˜ˆå€¼ {adjusted_threshold:.4f}")
            return True
        
        logger.info(f"âŒ è¯­ä¹‰åŒ¹é…å¤±è´¥ï¼šç»„åˆåˆ†æ•° {combined_score:.4f} < é˜ˆå€¼ {combined_threshold:.4f}")
        return False
    
    def _normalize_chinese_text(self, text: str) -> str:
        """
        ä¸­æ–‡æ–‡æœ¬æ ‡å‡†åŒ–å¤„ç†
        """
        if not text:
            return ""
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦
        import re
        text = re.sub(r'[^\u4e00-\u9fff]', '', text)
        
        # å»é™¤ç©ºæ ¼
        text = text.strip()
        
        return text
    
    def _calculate_linguistic_similarity(self, user_text: str, std_text: str) -> float:
        """
        è®¡ç®—è¯­è¨€å­¦ç›¸ä¼¼åº¦ï¼ŒåŸºäºä¸­æ–‡è¯­è¨€ç‰¹å¾
        """
        if not user_text or not std_text:
            return 0.0
        
        score = 0.0
        
        # 1. å­—ç¬¦é‡å åº¦ (æƒé‡: 0.4)
        user_chars = set(user_text)
        std_chars = set(std_text)
        if user_chars or std_chars:
            char_overlap = len(user_chars.intersection(std_chars)) / len(user_chars.union(std_chars))
            score += char_overlap * 0.4
        
        # 2. é•¿åº¦ç›¸ä¼¼åº¦ (æƒé‡: 0.2)
        len_user = len(user_text)
        len_std = len(std_text)
        if max(len_user, len_std) > 0:
            length_similarity = min(len_user, len_std) / max(len_user, len_std)
            score += length_similarity * 0.2
        
        # 3. å­ä¸²åŒ¹é… (æƒé‡: 0.3)
        substring_score = 0.0
        if user_text in std_text or std_text in user_text:
            substring_score = 1.0
        else:
            # æ£€æŸ¥éƒ¨åˆ†å­ä¸²åŒ¹é…
            for i in range(len(user_text)):
                for j in range(i + 1, len(user_text) + 1):
                    substr = user_text[i:j]
                    if len(substr) >= 2 and substr in std_text:
                        substring_score = max(substring_score, len(substr) / max(len_user, len_std))
        score += substring_score * 0.3
        
        # 4. è¯æ€§å˜åŒ–å®¹å¿ (æƒé‡: 0.1)
        # å¤„ç† "æ…¢æ€§" vs "æ…¢æ€§çš„" è¿™ç±»æƒ…å†µ
        if user_text + "çš„" == std_text or std_text + "çš„" == user_text:
            score += 0.1
        elif user_text.endswith("çš„") and user_text[:-1] in std_text:
            score += 0.1
        elif std_text.endswith("çš„") and std_text[:-1] in user_text:
            score += 0.1
        
        return min(1.0, score)
    
    def _calculate_answer_quality_penalty(self, user_answer: str) -> float:
        """
        è®¡ç®—ç­”æ¡ˆè´¨é‡æƒ©ç½šï¼Œç”¨äºè°ƒæ•´é˜ˆå€¼
        è¿”å›å€¼ï¼š0-0.2ä¹‹é—´ï¼Œè´¨é‡è¶Šå·®æƒ©ç½šè¶Šå¤§
        """
        penalty = 0.0
        
        # 1. æ£€æŸ¥æ— æ„ä¹‰å­—ç¬¦
        meaningless_chars = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')
        user_chars = set(user_answer)
        meaningless_ratio = len(meaningless_chars.intersection(user_chars)) / len(user_chars) if user_chars else 0
        penalty += meaningless_ratio * 0.15  # æœ€å¤šæƒ©ç½š0.15
        
        # 2. æ£€æŸ¥ç­”æ¡ˆé•¿åº¦åˆç†æ€§
        answer_len = len(user_answer.strip())
        if answer_len <= 1:
            penalty += 0.1  # è¿‡çŸ­ç­”æ¡ˆ
        elif answer_len > 20:
            penalty += 0.05  # è¿‡é•¿ç­”æ¡ˆå¯èƒ½ä¸å‡†ç¡®
        
        # 3. æ£€æŸ¥é‡å¤å­—ç¬¦
        if len(set(user_answer)) / len(user_answer) < 0.5 and len(user_answer) > 2:
            penalty += 0.05  # å­—ç¬¦é‡å¤åº¦è¿‡é«˜
        
        return min(0.2, penalty)  # æœ€å¤§æƒ©ç½š0.2
    
    def _get_length_factor(self, user_answer: str) -> float:
        """è·å–é•¿åº¦å› ç´  - å·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§"""
        return 1.0
    
    def _get_complexity_factor(self, user_answer: str) -> float:
        """è·å–å¤æ‚åº¦å› ç´  - å·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§"""
        return 1.0
    
    def _get_trend_factor(self, similarity: float) -> float:
        """è·å–è¶‹åŠ¿å› ç´  - å·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§"""
        return 1.0
    
    def _get_text_quality_factor(self, user_answer: str, best_match: str) -> float:
        """è·å–æ–‡æœ¬è´¨é‡å› ç´  - å·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§"""
        return 1.0
    
    def _calculate_dynamic_threshold(self, user_answer: str, best_match: str, similarity: float) -> float:
        """è®¡ç®—åŠ¨æ€é˜ˆå€¼ - å·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§"""
        return config.similarity_threshold
    
    def _check_and_preload_cache(self):
        """æ£€æŸ¥å¹¶é¢„è½½å…¥ç¼“å­˜"""
        if not config.api_key:
            logger.info("APIå¯†é’¥æœªé…ç½®ï¼Œè·³è¿‡ç¼“å­˜é¢„çƒ­")
            return
        
        # æ£€æŸ¥ç¼“å­˜ç»Ÿè®¡
        cache_stats = self.embedding_cache.get_stats()
        cache_size = cache_stats['cache_size']
        
        # å¦‚æœç¼“å­˜ä¸ºç©ºæˆ–å¾ˆå°ï¼Œæç¤ºç”¨æˆ·æ˜¯å¦é¢„çƒ­
        if cache_size < len(self.vocabulary) * 0.5:  # å°‘äº50%çš„è¯æ±‡è¢«ç¼“å­˜
            logger.info(f"å½“å‰ç¼“å­˜å¤§å°: {cache_size}, è¯æ±‡è¡¨å¤§å°: {len(self.vocabulary)}")
            logger.info("å»ºè®®ä½¿ç”¨ preload_cache() æ–¹æ³•é¢„çƒ­ç¼“å­˜ä»¥æå‡æ€§èƒ½")
    
    def preload_cache(self, max_words: int = None, batch_size: int = 10):
        """
        é¢„è½½å…¥è¯æ±‡è¡¨çš„embeddingåˆ°ç¼“å­˜
        
        Args:
            max_words: æœ€å¤§é¢„è½½å…¥è¯æ±‡æ•°ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
            batch_size: æ‰¹æ¬¡å¤§å°ï¼Œæ§åˆ¶APIè°ƒç”¨é¢‘ç‡
        """
        if not config.api_key:
            logger.error("APIå¯†é’¥æœªé…ç½®ï¼Œæ— æ³•é¢„çƒ­ç¼“å­˜")
            return False
        
        if not self.vocabulary:
            self.load_vocabulary()
        
        if not self.vocabulary:
            logger.error("è¯æ±‡è¡¨ä¸ºç©ºï¼Œæ— æ³•é¢„çƒ­ç¼“å­˜")
            return False
        
        # ç¡®å®šè¦é¢„è½½å…¥çš„è¯æ±‡
        vocab_to_preload = self.vocabulary
        if max_words and max_words < len(self.vocabulary):
            vocab_to_preload = self.vocabulary[:max_words]
        
        logger.info(f"å¼€å§‹é¢„çƒ­ç¼“å­˜ï¼š{len(vocab_to_preload)} ä¸ªè¯æ±‡")
        
        preloaded_count = 0
        api_calls = 0
        skipped_count = 0  # å·²å­˜åœ¨äºç¼“å­˜ä¸­çš„æ•°é‡
        
        import time
        
        for i, word_obj in enumerate(vocab_to_preload):
            word = word_obj.get('word', '')
            meanings = word_obj.get('meanings', [])
            
            if not word:
                continue
            
            try:
                # é¢„è½½å…¥è‹±æ–‡å•è¯
                if self.embedding_cache.get(word) is None:
                    embedding = self.get_embedding(word, "en")
                    if embedding is not None:
                        preloaded_count += 1
                        api_calls += 1
                else:
                    skipped_count += 1
                
                # é¢„è½½å…¥ä¸­æ–‡é‡Šä¹‰
                for meaning in meanings:
                    if meaning and self.embedding_cache.get(meaning) is None:
                        embedding = self.get_embedding(meaning, "zh")
                        if embedding is not None:
                            preloaded_count += 1
                            api_calls += 1
                    elif meaning:
                        skipped_count += 1
                
                # æ‰¹æ¬¡æ§åˆ¶ï¼šæ¯å¤„ç†batch_sizeä¸ªè¯æ±‡åä¼‘æ¯å¹¶æ›´æ–°ç»Ÿè®¡
                if (i + 1) % batch_size == 0:
                    progress = (i + 1) / len(vocab_to_preload) * 100
                    logger.info(f"è¿›åº¦ {progress:.1f}% - å·²å¤„ç† {i + 1}/{len(vocab_to_preload)} ä¸ªè¯æ±‡")
                    logger.info(f"  æ–°å¢: {preloaded_count}, å·²å­˜åœ¨: {skipped_count}, APIè°ƒç”¨: {api_calls}")
                    
                    # æ‰¹æ¬¡ä¿å­˜ç¼“å­˜
                    try:
                        self.embedding_cache._save_cache()
                    except Exception as e:
                        logger.warning(f"æ‰¹æ¬¡ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
                    
                    time.sleep(0.5)  # é€‚åº¦å»¶é²ï¼Œé¿å…APIè¯·æ±‚è¿‡äºé¢‘ç¹
                    
            except Exception as e:
                logger.error(f"é¢„è½½å…¥è¯æ±‡ '{word}' æ—¶å‡ºé”™: {e}")
                continue
        
        # æœ€ç»ˆä¿å­˜ç¼“å­˜
        try:
            self.embedding_cache._save_cache()
        except Exception as e:
            logger.warning(f"æœ€ç»ˆä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
        
        # è·å–æœ€ç»ˆç»Ÿè®¡
        final_stats = self.embedding_cache.get_stats()
        
        logger.info(f"ğŸ‰ ç¼“å­˜é¢„çƒ­å®Œæˆï¼")
        logger.info(f"  ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        logger.info(f"    æ–°å¢embedding: {preloaded_count}")
        logger.info(f"    è·³è¿‡(å·²å­˜åœ¨): {skipped_count}")
        logger.info(f"    APIè°ƒç”¨æ¬¡æ•°: {api_calls}")
        logger.info(f"    ç¼“å­˜æ€»å¤§å°: {final_stats['cache_size']}")
        logger.info(f"    å½“å‰å‘½ä¸­ç‡: {final_stats['hit_rate']}")
        
        # è®¡ç®—æ•ˆç‡æå‡é¢„ä¼°
        total_possible_items = len(vocab_to_preload) * 2  # è‹±æ–‡+ä¸­æ–‡
        coverage_rate = final_stats['cache_size'] / total_possible_items * 100 if total_possible_items > 0 else 0
        logger.info(f"    è¯æ±‡è¦†ç›–ç‡: {coverage_rate:.1f}%")
        
        if coverage_rate > 80:
            logger.info("âœ… ç¼“å­˜é¢„çƒ­æ•ˆæœè‰¯å¥½ï¼Œæµ‹è¯•å“åº”é€Ÿåº¦å°†å¤§å¹…æå‡ï¼")
        elif coverage_rate > 50:
            logger.info("âš¡ ç¼“å­˜é¢„çƒ­æ•ˆæœä¸­ç­‰ï¼Œå»ºè®®ç»§ç»­é¢„çƒ­æ›´å¤šè¯æ±‡")
        else:
            logger.info("âš ï¸  ç¼“å­˜è¦†ç›–ç‡è¾ƒä½ï¼Œå¯è€ƒè™‘å¢åŠ é¢„çƒ­è¯æ±‡æ•°é‡")
        
        return True
    
    def get_cache_info(self):
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        stats = self.embedding_cache.get_stats()
        vocab_size = len(self.vocabulary) if self.vocabulary else 0
        
        return {
            "cache_size": stats['cache_size'],
            "vocabulary_size": vocab_size,
            "coverage_rate": f"{stats['cache_size'] / (vocab_size * 2) * 100:.1f}%" if vocab_size > 0 else "0%",
            "hit_rate": stats['hit_rate'],
            "hits": stats['hits'],
            "misses": stats['misses']
        }

    def run_test(self, num_questions: int, on_question_display, on_result_display):
        """è¿è¡ŒIELTSæµ‹è¯•ä¼šè¯ã€‚"""
        # å¼€å§‹æ€§èƒ½ç›‘æ§ä¼šè¯
        self.performance_monitor.start_session("IELTS è‹±è¯‘ä¸­")
        
        if not self.vocabulary:
            on_result_display("é”™è¯¯ï¼šIELTSè¯æ±‡è¡¨ä¸ºç©ºæˆ–åŠ è½½å¤±è´¥ï¼Œæ— æ³•å¼€å§‹æµ‹è¯•ã€‚", True, 0, 0, 0, 0, [])
            return

        actual_num_questions = min(num_questions, len(self.vocabulary))
        if actual_num_questions <= 0:
            on_result_display("é”™è¯¯ï¼šæ²¡æœ‰è¶³å¤Ÿçš„è¯æ±‡è¿›è¡Œæµ‹è¯•ï¼ˆæˆ–è¯·æ±‚æ•°é‡ä¸º0ï¼‰ã€‚", True, 0, 0, 0, 0, [])
            return
            
        # ç¡®ä¿ random.sample ä¸ä¼šè¶…å‡ºèŒƒå›´
        if len(self.vocabulary) < actual_num_questions:
            logger.warning(f"è¯·æ±‚äº†{actual_num_questions}é¢˜ï¼Œä½†åªæœ‰{len(self.vocabulary)}ä¸ªå•è¯ã€‚å°†ä½¿ç”¨å…¨éƒ¨å•è¯ã€‚")
            actual_num_questions = len(self.vocabulary)

        selected_words = random.sample(self.vocabulary, actual_num_questions)
        
        # è¯»å–å®Œæ•´çš„è¯æ¡å¯¹è±¡åˆ—è¡¨ï¼ˆå¸¦ meaningsï¼‰
        json_path = resource_path("vocab/ielts_vocab.json")
        with open(json_path, 'r', encoding='utf-8') as file:
            vocab_data = json.load(file)
        word2meanings = {}
        if isinstance(vocab_data, list):
            for item in vocab_data:
                if isinstance(item, dict) and 'word' in item and 'meanings' in item:
                    word2meanings[item['word']] = item['meanings']
        
        correct_answers = 0
        incorrect_answers = 0
        skipped_answers = 0
        detailed_results = []

        for i, word_obj in enumerate(selected_words):
            english_word = word_obj['word']
            meanings = word_obj.get('meanings', [])
            question_text = f"è‹±æ–‡å•è¯ï¼š {english_word}\n\nè¯·è¾“å…¥æ‚¨çš„ä¸­æ–‡ç¿»è¯‘ï¼š"
            user_input = on_question_display(
                question_text, 
                i + 1, 
                actual_num_questions,
                is_semantic_test=True
            )
            if not meanings or not any(meanings):
                ref_answer = "ï¼ˆæ— ä¸­æ–‡é‡Šä¹‰ï¼‰"
            else:
                ref_answer = "ï¼›".join([m for m in meanings if m])
            if user_input is None:  # è·³è¿‡
                skipped_answers += 1
                result = TestResult(
                    question_num=i + 1,
                    question=english_word,
                    expected_answer=ref_answer,
                    user_answer="è·³è¿‡",
                    is_correct=False,
                    notes="ç”¨æˆ·è·³è¿‡"
                )
            elif not user_input.strip(): # ç©ºç­”æ¡ˆè§†ä¸ºé”™è¯¯
                incorrect_answers += 1
                result = TestResult(
                    question_num=i + 1,
                    question=english_word,
                    expected_answer=ref_answer,
                    user_answer=user_input if user_input else "<ç©º>",
                    is_correct=False,
                    notes="ç­”æ¡ˆä¸ºç©º"
                )
            else:
                is_correct = self.check_answer_with_api(meanings, user_input.strip())
                # è®°å½•ç­”é¡Œçµæœ
                self.performance_monitor.record_question_answered(is_correct)
                if is_correct:
                    correct_answers += 1
                else:
                    incorrect_answers += 1
                result = TestResult(
                    question_num=i + 1,
                    question=english_word,
                    expected_answer=ref_answer,
                    user_answer=user_input,
                    is_correct=is_correct,
                    notes=f"è¯­ä¹‰ç›¸ä¼¼åº¦åˆ¤å®š"
                )
            detailed_results.append(result)

        total_answered = correct_answers + incorrect_answers
        accuracy = (correct_answers / total_answered * 100) if total_answered > 0 else 0
        
        summary_message = (
            f"IELTSæµ‹è¯•å®Œæˆ!\n\n"
            f"æ€»é¢˜æ•°: {actual_num_questions}\n"
            f"å›ç­”æ­£ç¡®: {correct_answers}\n"
            f"å›ç­”é”™è¯¯: {incorrect_answers}\n"
            f"è·³è¿‡é¢˜æ•°: {skipped_answers}\n"
            f"å‡†ç¡®ç‡: {accuracy:.2f}% (åŸºäºå·²å›ç­”é¢˜ç›®)\n\n"
            f"æ³¨æ„ï¼šæ­¤æµ‹è¯•é€šè¿‡è¯­ä¹‰ç›¸ä¼¼åº¦åˆ¤æ–­ç­”æ¡ˆï¼Œé˜ˆå€¼ä¸º {config.similarity_threshold}ã€‚"
        )
        
        # ç»“æŸæ€§èƒ½ç›‘æ§ä¼šè¯
        self.performance_monitor.end_session()
        
        # æ›´æ–°ç¼“å­˜ç»Ÿè®¡
        cache_info = self.get_cache_info()
        self.performance_monitor.update_cache_stats(cache_info)
        
        on_result_display(
            summary_message, 
            False, 
            actual_num_questions, 
            correct_answers, 
            incorrect_answers, 
            skipped_answers, 
            detailed_results
        )

    def get_vocabulary_size(self) -> int:
        return len(self.vocabulary)

    def start(self, num_questions: int | None = None):
        """Runs the IELTS test in CLI mode."""
        if not self.vocabulary:
            self.load_vocabulary()
        
        if not self.vocabulary:
            logger.error("é”™è¯¯ï¼šIELTS è¯æ±‡è¡¨ä¸ºç©ºåŠ è½½å¤±è´¥ï¼Œæ— æ³•å¼€å§‹æµ‹è¯•ã€‚")
            return

        if num_questions is None:
            while True:
                try:
                    num_input = input(f"è¯·è¾“å…¥æµ‹è¯•é¢˜æ•° (1-{len(self.vocabulary)}ï¼Œé»˜è®¤ä¸º10ï¼Œç›´æ¥æŒ‰ Enter ä½¿ç”¨é»˜è®¤å€¼): ").strip()
                    if not num_input:
                        num_questions = 10
                        break
                    num_questions = int(num_input)
                    if 1 <= num_questions <= len(self.vocabulary):
                        break
                    else:
                        # This is a CLI user prompt, so print is appropriate
                        print(f"è¯·è¾“å…¥1åˆ°{len(self.vocabulary)}ä¹‹é—´çš„æœ‰æ•ˆæ•°å­—ã€‚")
                except ValueError:
                    # This is a CLI user prompt, so print is appropriate
                    print("è¾“å…¥æ— æ•ˆï¼Œè¯·è¾“å…¥ä¸€ä¸ªæ•°å­—ã€‚")
        
        num_questions = min(max(1, num_questions), len(self.vocabulary))

        prepared_count = self.prepare_test_session(num_questions)
        if prepared_count == 0:
            logger.error("é”™è¯¯ï¼šæ— æ³•å‡†å¤‡IELTSæµ‹è¯•ä¼šè¯ (å¯èƒ½æ˜¯è¯æ±‡è¡¨ä¸ºç©ºæˆ–æ•°é‡ä¸è¶³)ã€‚")
            return

        # This is a CLI status message, print is appropriate
        print(f"--- å¼€å§‹IELTSè‹±è¯‘ä¸­æµ‹è¯• (å…± {prepared_count} é¢˜) ---")
        correct_answers = 0
        detailed_results_cli = []

        for i, word_obj in enumerate(self.selected_words_for_session):
            eng_word = word_obj['word']
            meanings = word_obj.get('meanings', [])
            print(f"\né¢˜ç›® {i+1}/{prepared_count}: {eng_word}")
            # CLI user interaction - print is appropriate
            user_chinese = input("è¯·è¾“å…¥æ‚¨çš„ä¸­æ–‡ç¿»è¯‘: ").strip()
            is_correct = False
            if not user_chinese:
                # CLI user feedback - print is appropriate
                print("æç¤ºï¼šç­”æ¡ˆä¸èƒ½ä¸ºç©ºï¼Œè®¡ä¸ºé”™è¯¯ã€‚")
            else:
                is_correct = self.check_answer_with_api(meanings, user_chinese)
            if is_correct:
                # CLI user feedback - print is appropriate
                print("å›ç­”æ­£ç¡®ï¼")
                correct_answers += 1
            else:
                # CLI user feedback - print is appropriate
                print("å›ç­”é”™è¯¯æˆ–è¯­ä¹‰ä¸ç¬¦ã€‚")
            detailed_results_cli.append(
                TestResult(
                    question_num=i + 1,
                    question=eng_word,
                    expected_answer=f"è¯­ä¹‰ç›¸ä¼¼åº¦ > {config.similarity_threshold}",
                    user_answer=user_chinese if user_chinese else "<ç©º>",
                    is_correct=is_correct,
                    notes="è¯­ä¹‰ç›¸ä¼¼åº¦åˆ¤å®š (CLI)"
                )
            )

        print("\n" + "="*20 + " æµ‹è¯•ç»“æŸ " + "="*20)
        print(f"æ€»é¢˜æ•°: {prepared_count}")
        print(f"å›ç­”æ­£ç¡®: {correct_answers}")
        print(f"å›ç­”é”™è¯¯: {prepared_count - correct_answers}")
        accuracy = (correct_answers / prepared_count * 100) if prepared_count > 0 else 0
        # CLI summary - print is appropriate
        print(f"å‡†ç¡®ç‡: {accuracy:.2f}%")
        print(f"(è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼: {config.similarity_threshold})")

        # Optionally, display wrong answers - CLI output
        if correct_answers < prepared_count:
            print("\n--- é”™è¯¯é¢˜ç›®å›é¡¾ ---")
            for res in detailed_results_cli:
                if not res.is_correct:
                    print(f"é¢˜ç›®: {res.question}")
                    print(f"  æ‚¨çš„ç­”æ¡ˆ: {res.user_answer}")
                    print(f"  åˆ¤å®šæ ‡å‡†: {res.expected_answer}")
        print("="*50)
